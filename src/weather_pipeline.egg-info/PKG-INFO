Metadata-Version: 2.4
Name: weather-pipeline
Version: 0.1.0
Summary: Production-ready weather ETL pipeline
Author-email: Seu Nome <seu@email.com>
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.31
Requires-Dist: pandas>=2.1
Requires-Dist: pydantic-settings>=2.0
Requires-Dist: python-dotenv>=1.0
Requires-Dist: structlog>=23.1
Provides-Extra: dev
Requires-Dist: pytest>=7.4; extra == "dev"
Requires-Dist: pytest-cov>=4.1; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: mypy>=1.8; extra == "dev"
Requires-Dist: pre-commit>=3.5; extra == "dev"
Requires-Dist: bandit[toml]>=1.7; extra == "dev"

# Weather Pipeline

This project is a data engineering pipeline using **Apache Airflow**, **Docker**, and **PostgreSQL** to extract weather data from the OpenWeather API.

## 🔧 Technologies Used

- Python 3.8+
- Apache Airflow 2.8.1 (with LocalExecutor)
- Docker + Docker Compose
- PostgreSQL (two separate databases: metadata and weather data)
- OpenWeather API (Current Weather Data)

## 📁 Project Structure

weather_pipeline/
├── dags/
│   └── weather_pipeline.py     # Main Airflow DAG
├── plugins/
├── data/
├── logs/
├── .env                        # Keys and secrets (do not version)
├── .gitignore
├── docker-compose.yaml         # Container orchestration
└── README.md

## 🚀 Current Features

- Extracts current weather data for specific coordinates
- Data is retrieved directly from the OpenWeather API
- Pipeline is executed through a scheduled DAG in Airflow

## 🔑 .env Example

FERNET_KEY=YourFernetKeyGeneratedHere=
OPENWEATHER_API_KEY=your_api_key_here

## ▶️ How to Run Locally

1. Generate your Fernet key:

python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

2. Create a `.env` file with your keys.

3. Start the services with Docker:

docker-compose up -d

4. Initialize the metadata database and create the admin user:

docker-compose exec airflow-webserver airflow db migrate

docker-compose exec airflow-webserver airflow users create \\
  --username admin \\
  --firstname Admin \\
  --lastname User \\
  --role Admin \\
  --email admin@example.com \\
  --password admin

5. Access Airflow at http://localhost:8080 and activate the weather_data_pipeline DAG.

## 📌 Next Steps

- Add a task for data transformation and cleaning
- Persist data in the `weather_data` database
- Create visualizations with Power BI or Streamlit

---

Project developed by nannayusuf (https://github.com/nannayusuf) as part of a data engineering portfolio.
"""

# Salvar em arquivo .txt
file_path = Path("/mnt/data/README_weather_pipeline.txt")
file_path.write_text(readme_txt_content)

file_path.name
